import logging

from bs4 import BeautifulSoup
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem
from scrapy.selector import Selector
from sqlalchemy.orm import sessionmaker

from scrapy_crawler.util.db.models import RawUsedItem
from scrapy_crawler.util.db.settings import get_engine


class DuplicateFilterPipeline:
    name = "DuplicateFilterPipeline"

    def __init__(self):
        self.Session = sessionmaker(bind=get_engine())
        self.session = self.Session()

    def is_duplicated(self, adapter: ItemAdapter) -> bool:
        item = (
            self.session.query(RawUsedItem)
            .filter(RawUsedItem.url == adapter["url"])
            .filter(
                RawUsedItem.writer == adapter["writer"]
                and RawUsedItem.title == adapter["title"]
            )
            .first()
        )

        return item is not None

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        if self.is_duplicated(adapter):
            raise DropItem("Duplicate item found: %s" % item)
        return item


class ContentScraperPipeline:
    name = "ContentScraperPipeline"

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        selector = Selector(text=adapter["content"])

        content = (
            BeautifulSoup("\n".join(selector.css(".se-text-paragraph > span").getall()))
            .get_text()
            .replace("â€‹", "")
            .replace("ğŸ‘†ì¤‘ê³ ë‚˜ë¼ ì•±ì´ ìˆë‹¤ëŠ” ê±¸ ì•„ì‹œë‚˜ìš”?", "")
            .replace("ìƒë‹¨ ì¤‘ê³ ë‚˜ë¼ ì•± ë‹¤ìš´ë°›ê¸° í´ë¦­!", "")
            .replace("ğŸ‘†ì•±ì—ì„œ êµ¬ë§¤ë¥¼ ì›í•˜ëŠ” ëŒ“ê¸€ì´ ë‹¬ë¦´ ìˆ˜ë„ ìˆì–´ìš”! ë”ë³´ê¸° í´ë¦­í•˜ê³  ë¯¸ë¦¬ ì•Œì•„ë‘ê¸°!", "")
            .replace(
                "â€» ë“±ë¡í•œ ê²Œì‹œê¸€ì´ íšŒì›ì˜ ì‹ ê³ ë¥¼ ë°›ê±°ë‚˜ ì´ìƒê±°ë˜ë¡œ ëª¨ë‹ˆí„°ë§ ë  ê²½ìš° ì¤‘ê³ ë‚˜ë¼ ì‚¬ê¸°í†µí•©ì¡°íšŒ DBë¡œ ìˆ˜ì§‘/í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "",
            )
            .replace("â€» ìœ íŠœë¸Œ, ë¸”ë¡œê·¸, ì¸ìŠ¤íƒ€ê·¸ë¨ ë“± ìƒí’ˆ ì •ë³´ ì œê³µ ëª©ì  ë§í¬ ê°€ëŠ¥(ì™¸ë¶€ ê±°ë˜ë¥¼ ìœ ë„í•˜ëŠ” ë§í¬ ì œì™¸) ", "")
            .replace("â”€", "")
            .replace("\n\n", "")
        )

        images = selector.css(".se-image-resource::attr(src)").getall()

        item["content"] = content + "\n[Image URLS]\n" + "\n".join(images)

        return item


class ManualFilterPipeline:
    name = "ManualFilterPipeline"

    def __init__(self):
        self.forbidden_words = ["ë§¤ì…", "ì‚½ë‹ˆë‹¤", "êµí™˜", "íŒŒíŠ¸ë„ˆ"]
        self.price_threshold = 200000

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        title = adapter["title"]
        price = adapter["price"]
        content = adapter["content"]

        if self.forbidden_words in title or self.forbidden_words in content:
            raise DropItem("Forbidden word found: %s" % item)

        if price <= self.price_threshold:
            raise DropItem("Price is too low: %s" % item)

        return item


class PostgresPipeline:
    name = "postgres_pipeline"

    def __init__(self):
        self.session = sessionmaker(bind=get_engine())()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        try:
            self.session.add(
                RawUsedItem(
                    url=adapter["url"],
                    img_url=adapter["img_url"],
                    price=adapter["price"],
                    date=adapter["date"],
                    writer=adapter["writer"],
                    title=adapter["title"],
                    content=adapter["content"],
                    source="ì¤‘ê³ ë‚˜ë¼",
                )
            )
            self.session.commit()
        except Exception as e:
            logging.error(e)
            self.session.rollback()

        return item
