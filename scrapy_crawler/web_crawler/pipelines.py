# -*- coding: utf-8 -*-
import logging

from bs4 import BeautifulSoup
from itemadapter import ItemAdapter

from scrapy_crawler.util.db.Postgres import PostgresClient
from scrapy.exceptions import DropItem
from scrapy.selector import Selector


class DuplicateFilterPipeline:
    name = "DuplicateFilterPipeline"

    def __init__(self):
        self.db = PostgresClient()
        self.cur = self.db.getCursor()

    def url_already_exists(self, url):
        self.cur.execute("SELECT * FROM macguider.raw_used_item WHERE url = %s", (url,))
        return self.cur.fetchone()

    def title_already_exists(self, title):
        self.cur.execute("SELECT * FROM macguider.raw_used_item WHERE title = %s", (title,))
        return self.cur.fetchone()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        if self.url_already_exists(adapter["url"]) or self.title_already_exists(adapter["title"]):
            raise DropItem("Duplicate item found: %s" % item)
        return item


class ContentScraperPipeline:
    name = "ContentScraperPipeline"

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        selector = Selector(text=adapter["content"])

        content = BeautifulSoup("\n".join(selector.css(".se-text-paragraph > span").getall())).get_text() \
            .replace("â€‹", "") \
            .replace("ğŸ‘†ì¤‘ê³ ë‚˜ë¼ ì•±ì´ ìˆë‹¤ëŠ” ê±¸ ì•„ì‹œë‚˜ìš”? ìƒë‹¨ ì¤‘ê³ ë‚˜ë¼ ì•± ë‹¤ìš´ë°›ê¸° í´ë¦­!", "") \
            .replace("ğŸ‘†ì•±ì—ì„œ êµ¬ë§¤ë¥¼ ì›í•˜ëŠ” ëŒ“ê¸€ì´ ë‹¬ë¦´ ìˆ˜ë„ ìˆì–´ìš”! ë”ë³´ê¸° í´ë¦­í•˜ê³  ë¯¸ë¦¬ ì•Œì•„ë‘ê¸°!", "") \
            .replace("â€» ë“±ë¡í•œ ê²Œì‹œê¸€ì´ íšŒì›ì˜ ì‹ ê³ ë¥¼ ë°›ê±°ë‚˜ ì´ìƒê±°ë˜ë¡œ ëª¨ë‹ˆí„°ë§ ë  ê²½ìš° ì¤‘ê³ ë‚˜ë¼ ì‚¬ê¸°í†µí•©ì¡°íšŒ DBë¡œ ìˆ˜ì§‘/í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "") \
            .replace("â”€", "").replace("\n\n", "")

        images = selector.css(".se-image-resource::attr(src)").getall()

        item["content"] = content + "\n[Image URLS]\n" + "\n".join(images)

        return item


class PostgresPipeline:
    name = "postgres_pipeline"

    def __init__(self):
        self.db = PostgresClient()
        self.cur = self.db.getCursor()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        try:
            self.cur.execute(
                "INSERT INTO macguider.raw_used_item (url, img_url, price, date, writer, title, content, source) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)",
                (adapter["url"], adapter["img_url"], adapter["price"], adapter["date"], adapter["writer"],
                 adapter["title"], adapter["content"], "ì¤‘ê³ ë‚˜ë¼"))
            self.db.commit()
        except Exception as e:
            logging.error(e)
            self.db.rollback()

        return item
