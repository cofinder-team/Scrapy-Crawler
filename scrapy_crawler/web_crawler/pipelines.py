import logging

from bs4 import BeautifulSoup
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem
from scrapy.selector import Selector
from sqlalchemy.orm import sessionmaker

from scrapy_crawler.util.db.models import RawUsedItem
from scrapy_crawler.util.db.settings import get_engine


class DuplicateFilterPipeline:
    name = "DuplicateFilterPipeline"

    def __init__(self):
        self.session = sessionmaker(bind=get_engine())()

    def is_duplicated(self, adapter: ItemAdapter) -> bool:
        item = None
        source = adapter["source"]

        if source == "Ï§ëÍ≥†ÎÇòÎùº":
            item = (
                self.session.query(RawUsedItem)
                .filter(RawUsedItem.url == adapter["url"])
                .first()
            )
        elif source == "Î≤àÍ∞úÏû•ÌÑ∞":
            item = (
                self.session.query(RawUsedItem)
                .filter(
                    RawUsedItem.url
                    == f"https://api.bunjang.co.kr/api/pms/v2/products-detail/{adapter['pid']}?viewerUid=-1"
                )
                .first()
            )

        return item is not None

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        if self.is_duplicated(adapter):
            raise DropItem("Duplicate item found: %s" % item)
        return item


class ContentScraperPipeline:
    name = "ContentScraperPipeline"

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        selector = Selector(text=adapter["content"])

        content = (
            BeautifulSoup("\n".join(selector.css(".se-text-paragraph > span").getall()))
            .get_text()
            .replace("‚Äã", "")
            .replace("üëÜÏ§ëÍ≥†ÎÇòÎùº Ïï±Ïù¥ ÏûàÎã§Îäî Í±∏ ÏïÑÏãúÎÇòÏöî?", "")
            .replace("ÏÉÅÎã® Ï§ëÍ≥†ÎÇòÎùº Ïï± Îã§Ïö¥Î∞õÍ∏∞ ÌÅ¥Î¶≠!", "")
            .replace("üëÜÏï±ÏóêÏÑú Íµ¨Îß§Î•º ÏõêÌïòÎäî ÎåìÍ∏ÄÏù¥ Îã¨Î¶¥ ÏàòÎèÑ ÏûàÏñ¥Ïöî! ÎçîÎ≥¥Í∏∞ ÌÅ¥Î¶≠ÌïòÍ≥† ÎØ∏Î¶¨ ÏïåÏïÑÎëêÍ∏∞!", "")
            .replace(
                "‚Äª Îì±Î°ùÌïú Í≤åÏãúÍ∏ÄÏù¥ ÌöåÏõêÏùò Ïã†Í≥†Î•º Î∞õÍ±∞ÎÇò Ïù¥ÏÉÅÍ±∞ÎûòÎ°ú Î™®ÎãàÌÑ∞ÎßÅ Îê† Í≤ΩÏö∞ Ï§ëÍ≥†ÎÇòÎùº ÏÇ¨Í∏∞ÌÜµÌï©Ï°∞Ìöå DBÎ°ú ÏàòÏßë/ÌôúÏö©Îê† Ïàò ÏûàÏäµÎãàÎã§.",
                "",
            )
            .replace("‚Äª Ïú†ÌäúÎ∏å, Î∏îÎ°úÍ∑∏, Ïù∏Ïä§ÌÉÄÍ∑∏Îû® Îì± ÏÉÅÌíà Ï†ïÎ≥¥ Ï†úÍ≥µ Î™©Ï†Å ÎßÅÌÅ¨ Í∞ÄÎä•(Ïô∏Î∂Ä Í±∞ÎûòÎ•º Ïú†ÎèÑÌïòÎäî ÎßÅÌÅ¨ Ï†úÏô∏) ", "")
            .replace("‚îÄ", "")
            .replace("\n\n", "")
        )

        images = selector.css(".se-image-resource::attr(src)").getall()

        item["content"] = content + "\n[Image URLS]\n" + "\n".join(images)

        return item


class ManualFilterPipeline:
    name = "ManualFilterPipeline"

    def __init__(self):
        self.forbidden_words = ["Îß§ÏûÖ", "ÏÇΩÎãàÎã§", "ÌååÌä∏ÎÑà"]
        self.price_threshold = 200000

    def has_forbidden_word(self, title: str, content: str) -> bool:
        return any(word in title for word in self.forbidden_words) or any(
            word in content for word in self.forbidden_words
        )

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        source = adapter["source"]
        title: str = ""
        content: str = ""

        if source == "Î≤àÍ∞úÏû•ÌÑ∞":
            title = adapter["name"]
            content = adapter["description"]
        elif source == "Ï§ëÍ≥†ÎÇòÎùº":
            title = adapter["title"]
            content = adapter["content"]
        price = adapter["price"]

        if self.has_forbidden_word(title, content):
            raise DropItem("Forbidden word found: %s" % item)

        if price <= self.price_threshold:
            raise DropItem("Price is too low: %s" % item)

        return item


class PostgresPipeline:
    name = "postgres_pipeline"

    def __init__(self):
        self.session = sessionmaker(bind=get_engine())()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        source = adapter["source"]
        url, img_url, price, date, writer, title, content = "", "", "", "", "", "", ""

        if source == "Î≤àÍ∞úÏû•ÌÑ∞":
            url = f"https://api.bunjang.co.kr/api/pms/v2/products-detail/{adapter['pid']}?viewerUid=-1"
            img_url = adapter["imageUrl"]
            price = adapter["price"]
            date = adapter["updatedAt"]
            writer = adapter["writer"]
            title = adapter["name"]
            content = adapter["description"]
        elif source == "Ï§ëÍ≥†ÎÇòÎùº":
            url = adapter["url"]
            img_url = adapter["img_url"]
            price = adapter["price"]
            date = adapter["date"]
            writer = adapter["writer"]
            title = adapter["title"]
            content = adapter["content"]

        try:
            self.session.add(
                RawUsedItem(
                    url=url,
                    img_url=img_url,
                    price=price,
                    date=date,
                    writer=writer,
                    title=title,
                    content=content,
                    source=source,
                )
            )
            self.session.commit()
        except Exception as e:
            logging.error(e)
            self.session.rollback()

        return item
